meta:
  data_name: 'mnist'
  experiment_name: 'trial_00'
  # TODO: information on when to save params, currently only best params saved
logging:
  console:
    level: 'info'
    format_str: null
  file:
    level: 'ERROR'
    format_str: null
  graph_spec: True

performance:
  objectives:
    # each objective can be a loss, a metric, or a combination of both
    main_obj:
      loss:
        # each loss can only be one loss
        # TODO: support custom losses
        type: 'sparse_categorical_crossentropy'
        track: "mean"
        #options:
      metric:
        # there can be many metrics
        # TODO: support custom metrics
        type: ["SparseCategoricalAccuracy"]
        options: [null]
      in_config:
        type: "supervised"
        options:
          prediction: "y_pred"
          target: "y_target"
        dataset: 'mnist'
    # second_obj:
    #   metric:
    #     type: ["TopKCategoricalAccuracy", "TopKCategoricalAccuracy"]
    #     options: [{"k": 2}, {"k": 3}]
    #   in_config:
    #     type: "supervised"
    #     options:
    #       prediction: "y_pred"
    #       target: "y_target"
    #     dataset: 'mnist'

# TODO: this section needs to be redone
data:
  datasets:
    "mnist":
      in:
        x_image:
          shape: [28,28,1]
          dtype: 'float32' # this is a cast
        y_target:
          shape: [1,1]
          dtype: 'int32'
          label: True
      split:
        names: ["train", "val"]

optimize:
  # NOTE: multiple losses by the same optimizer, are currently only modeled
  # jointly, if we wish to model the losses seperately (sequentially or
  # alternating), then we would want to use a second optimizer
  optimizers:
    "main_opt":
      type: 'adam'
      options:
        learning_rate: 0.0001
      objectives: ["main_obj"]
  directive:
    instructions: "main_opt"

hyper_parameters:
  epochs: 3
  dataset:
    # TODO: I would like to make this logic more abstract
    # I think the only options that should be applied here are "batch" and "shuffle"
    batch: 16
    shuffle_buffer: 128 # this should be grouped with batchsize
  # other:
  #   earlystopping:
  #     type: 'earlystopping'
  #     options:

model:
  path: './model_config.yml'

# current config (3 epochs)
# '''
# CPU times: user 3min 17s, sys: 18.3 s, total: 3min 35s
# Wall time: 1min 28s
# '''
# '''
# CPU times: user 3min 6s, sys: 16.7 s, total: 3min 23s
# Wall time: 1min 21s
# '''